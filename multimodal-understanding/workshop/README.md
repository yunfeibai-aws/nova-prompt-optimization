# Amazon Nova Multimodal understanding workshop

Amazon Nova is a new generation of state-of-the-art (SOTA) foundation models (FMs) that deliver frontier intelligence and industry leading price-performance, available exclusively on Amazon Bedrock.

Amazon Nova Micro, Amazon Nova Lite, and Amazon Nova Pro are understanding models that accept text, image, or video inputs and generate text output. They provide a broad selection of capability, accuracy, speed, and cost operation points.

Fast and cost-effective inference across intelligence classes State-of-the-art text, image, and video understanding
Fine-tuning on text, image, and video input Leading agentic and multimodal retrieval augmented generation (RAG) capabilities
Easy integration to proprietary data and applications with Amazon Bedrock
In this workshop, we will explore the Nova family of multimodal models. There are some prerequisites before we can all get started with the workshop.

## Setup

### Prerequisites

- Access to a SageMaker Studio Domain
- Familiarity with Jupyter notebooks

### Getting Started

1. Navigate to your [SageMaker Studio Domain](https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/studio)
2. Select **run** for the JupyterLab instance
3. Wait a few seconds, then click **open** to access
4. Clone this repo in your Sagemaker studio instance
5. Open the **multimodal-understanding/workshop** to complete the workshop

## Workshop Structure

Specifically we will go through the following labs:

Module 1: Exploring Nova Models in Action via Bedrock Console, Invoke API, Converse API

Module 2: Prompt Engineering Best Practices with Nova Models

Module 3: Building a Multimodal RAG System

Module 4: Adding Agent Actions to Multimodal RAG System

## Additional Resources

- [Amazon Nova Foundation Models](https://aws.amazon.com/ai/generative-ai/nova/)
- [Amazon Nova User Guide](https://docs.aws.amazon.com/nova/latest/userguide/what-is-nova.html)

