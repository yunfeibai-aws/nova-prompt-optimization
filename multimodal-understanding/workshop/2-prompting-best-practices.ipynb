{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 2 - Prompting Best Practices with Amazon Nova Models\n",
    "\n",
    "The effectiveness of prompts is contingent upon the quality of the information provided and the craftsmanship of the prompt itself. Prompts may encompass instructions, questions, contextual details, inputs, and examples to effectively guide the model and enhance the quality of the results. This document outlines strategies and tactics for optimizing the performance of Amazon Nova Family of Models. The methods presented herein may be employed in various combinations to amplify their effectiveness. We encourage users to engage in experimentation to identify the approaches most suitable for their specific needs. More details on prompt best practices can be found [here](https://docs.aws.amazon.com/nova/latest/userguide/prompting-text-understanding.html)\n",
    "\n",
    "## Before Starting Prompt Engineering\n",
    "\n",
    "Before starting prompt engineering, it is highly recommended to have following elements in place, so you can iteratively develop the most optimal prompt for your use case:\n",
    "\n",
    "1. **Define your use case**: Define your use case you want to achieve on 4 dimensions\n",
    "    1. What is the Task - Define the task you want to achieve from the model\n",
    "    2. Whats the Role - Define the role model should act like to accomplish that task\n",
    "    3. Whats the Response Style - Define the response structure or style that should be followed based on the consumer of the output. \n",
    "    4. What set of Instructions to be followed:  Define the  set of instructions that model should follow to respond as per the success criteria\n",
    "2. **Success Criteria**: Clearly define the success criteria or evaluation criteria. This can be in the form of a list of bullet points or as specific as some evaluation metrics (Eg: Length checks, BLEU Score, Rouge, Format, Factuality, Faithfulness). The success criteria should clearly reflect the definition of \"good\" that will determine the success of the use case.\n",
    "3. **Draft Prompt**: Finally, a draft prompt is necessary to initiate the iterative process of prompt engineering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in variables and config from previous notebook\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import base64\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "boto3.setup_default_session(region_name=region_name)\n",
    "client = boto3.client(\"bedrock-runtime\")\n",
    "\n",
    "\n",
    "def call_nova(\n",
    "    model,\n",
    "    messages,\n",
    "    system_message=\"\",\n",
    "    streaming=False,\n",
    "    max_tokens=512,\n",
    "    temp=0.7,\n",
    "    top_p=0.99,\n",
    "    top_k=20,\n",
    "    tools=None,\n",
    "    stop_sequences=[],\n",
    "    verbose=False,\n",
    "):\n",
    "    system_list = [{\"text\": system_message}]\n",
    "    inf_params = {\n",
    "        \"max_new_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "        \"temperature\": temp,\n",
    "        \"stopSequences\": stop_sequences,\n",
    "    }\n",
    "    request_body = {\n",
    "        \"messages\": messages,\n",
    "        \"system\": system_list,\n",
    "        \"inferenceConfig\": inf_params,\n",
    "    }\n",
    "    if tools is not None:\n",
    "        tool_config = []\n",
    "        for tool in tools:\n",
    "            tool_config.append({\"toolSpec\": tool})\n",
    "        request_body[\"toolConfig\"] = {\"tools\": tool_config}\n",
    "    if verbose:\n",
    "        print(\"Request Body\", request_body)\n",
    "    if not streaming:\n",
    "        response = client.invoke_model(modelId=model, body=json.dumps(request_body))\n",
    "        model_response = json.loads(response[\"body\"].read())\n",
    "        return model_response, model_response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    else:\n",
    "        response = client.invoke_model_with_response_stream(\n",
    "            modelId=model, body=json.dumps(request_body)\n",
    "        )\n",
    "        return response[\"body\"]\n",
    "\n",
    "\n",
    "def get_base64_encoded_value(media_path):\n",
    "    with open(media_path, \"rb\") as media_file:\n",
    "        binary_data = media_file.read()\n",
    "        base_64_encoded_data = base64.b64encode(binary_data)\n",
    "        base64_string = base_64_encoded_data.decode(\"utf-8\")\n",
    "        return base64_string\n",
    "\n",
    "\n",
    "def print_output(content_text):\n",
    "    display(Markdown(content_text))\n",
    "\n",
    "\n",
    "def validate_json(json_string):\n",
    "    try:\n",
    "        # Attempt to parse the JSON string\n",
    "        parsed_json = json.loads(json_string)\n",
    "\n",
    "        # If successful, return the parsed JSON\n",
    "        print(\"Valid JSON\")\n",
    "        return parsed_json\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        # If parsing fails, print an error message\n",
    "        print(f\"Invalid JSON: {e}\")\n",
    "\n",
    "        # Optionally, you can print the location of the error\n",
    "        print(f\"Error at line {e.lineno}, column {e.colno}\")\n",
    "\n",
    "        # Return None to indicate failure\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Structured Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Sometimes it is essential to make sure that the model only responds in a specific output schema that works best for the downstream use cases (for example, automated workflows where inputs and outputs must always be in a specific format). Amazon Nova models can be instructed to generate responses in a structured way. For example, if the downstream parser expects certain naming of keys in the JSON object, specifying them in an output schema field in your query yields the model to respect that schema. If the need is to be able to parse the schema directly without any preamble, the model can be instructed to output only JSON by saying “Please generate only the JSON output. DO NOT provide any preamble.” at the end of your query. \n",
    "\n",
    "#### Using Prefill to Help the Model Get Started\n",
    "\n",
    "An alternate technique to achieve this efficiently is to nudge the model response via prefilling the assistant content. This technique enables the user to direct the model's actions (putting words in the model's mouth), bypass preambles, and enforce specific output formats such as JSON or XML. For example, by prefilling assistant content with “{” or ```json, you can guide model to skip generating any preamble text and start generating JSON object right away. \n",
    "\n",
    "\n",
    "➡️ If the user is explicitly looking for extracting JSON, one common observed pattern is to prefill it with ``json and add a stop sequence on ```, this ensures that the model outputs a JSON object that can be programmatically parsed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unoptimized_prompt = \"\"\"Provide details about the best selling full-frame cameras in past three years.\n",
    "Answer in JSON format with keys like name, brand, price and a summary.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [{\"text\": unoptimized_prompt}]},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response, content_text = call_nova(LITE_MODEL_ID, messages)\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(\"-\" * 40)\n",
    "print(content_text)\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Lets add more schema definition with the right data types and use Prefill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_prompt = \"\"\"Provide 5 examples of the best selling full-frame cameras in past three years.\n",
    "Follow the Output Schema as described below:\n",
    "Output Schema:\n",
    "{\n",
    "\"name\" : <string, the name of product>,\n",
    "\"brand\" : <string, the name of product>,\n",
    "\"price\" : <integer price>,\n",
    "\"summary\": <string, the product summary>\n",
    "}\n",
    "Only Respond in Valid JSON, without Markdown\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [{\"text\": optimized_prompt}]},\n",
    "    {\"role\": \"assistant\", \"content\": [{\"text\": \"```json\"}]},\n",
    "]\n",
    "\n",
    "model_response, content_text = call_nova(LITE_MODEL_ID, messages, stop_sequences=[\"]\"])\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(\"-\" * 40)\n",
    "print(content_text)\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"Testing valid JSON:\")\n",
    "parsed_json = validate_json(content_text)\n",
    "if parsed_json:\n",
    "    print(parsed_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Few Shot Example\n",
    "\n",
    "Including a few examples of the task within your prompt can help to guide Amazon Nova models to generate responses more aligned with your desired outcome. This technique of providing examples to the model to achieve the desired outcome is called few shot prompting. By including the examples using a structured template, you can enable the models to follow instructions, reduce ambiguity, and enhance the accuracy and quality more reliably. This method also helps in clarifying complex instructions or tasks, making it easier for the models to understand and interpret what is being asked. \n",
    "\n",
    "**How adding examples to the prompt help**:\n",
    "Adding examples can help the model with producing \n",
    "\n",
    "* Consistent responses which are uniform to the style of the examples \n",
    "* Performant responses due to reducing the chance of misinterpreting instructions, and minimizing hallucinations\n",
    "\n",
    "\n",
    "**Characteristics of Good Shots in prompt**:\n",
    "The amount by which model performance improves using few shot prompting will depend on the quality and diversity of your chosen examples. \n",
    "\n",
    "* **Select diverse examples**: The examples chosen should represent the distribution of your expected input/output in terms of diversity (ranging from common use cases to edge cases) to adequately cover relevant use cases. It is important to avoid any biases in your examples, as bias in the inputs can cause outputs to be biased as well.\n",
    "* **Match complexity levels**: The complexity of the examples provided should align with the target task or scenario. It is important to make sure the complexity grade is mapped between expected the input and the chosen example in the prompt.\n",
    "* **Ensure relevance**: The examples selected should be directly relevant to the problem or objective at hand. This ensures consistency and uniformity in responses. \n",
    "\n",
    "➡️ Tip: If the above suggestions not work, it is also recommended to build a RAG-based system that augments the prompt with dynamic selection of shots based on the similarities between a user-input query and an available pool of shots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_shot = \"\"\"Your task is to Classify the following texts into the appropriate sentiment classes. The categories to classify are:\n",
    "\n",
    "Sentiment Classes:\n",
    "- Positive\n",
    "- Negative\n",
    "- Neutral\n",
    "\n",
    "Query:\n",
    "Input: The movie makes users think about their lives with the teenagers while still making audience unclear on the storyline.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": no_shot}]}]\n",
    "\n",
    "model_response, content_text = call_nova(LITE_MODEL_ID, messages)\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(\"-\" * 40)\n",
    "print_output(content_text)\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "As you can see there is too much explaination not even asked for, this makes parsing a bit tricky \n",
    "\n",
    "Now lets try adding four shots that conveys the meaning and also forces a more stylistic edit on the response \n",
    "\n",
    "### With adding few shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "four_shot = \"\"\"Your task is to Classify the following texts into the appropriate sentiment classes. The categories to classify are:\n",
    "\n",
    "Sentiment Classes:\n",
    "- Positive\n",
    "- Negative\n",
    "- Neutral\n",
    "\n",
    "Please refer to some examples mentioned below.\n",
    "\n",
    "## Examples\n",
    "### Example 1\n",
    "Input: The movie was crazy good! I loved it\n",
    "Output: Positive\n",
    "Explaination: The text said \"good\" and \"loved\" so its positive\n",
    "\n",
    "### Example 2\n",
    "Input: The movie was scary and I got scared!\n",
    "Output: Neutral\n",
    "Explaination: The text said \"scary\" and \"scared\" which can be both positive and negative depending on people who like scary movies or one who hate\n",
    "\n",
    "### Example 3\n",
    "Input: The movie was pathetic not worth the time or money!\n",
    "Output: Negative\n",
    "Explaination: The text said \"pathetic\" and \"not worth\" which is negative sentiment\n",
    "\n",
    "### Example 4\n",
    "Input: The movie had some plots which were interesting and great while there were some gaps which needed more drama!\n",
    "Output: Neutral\n",
    "Explaination: The text said \"interesting and great\" and \"some gaps\" making it a mixed opinion hence neutral\n",
    "\n",
    "Query:\n",
    "Input: The movie makes users think about their lives with the teenagers while still making audience unclear on the storyline.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": four_shot}]}]\n",
    "\n",
    "model_response, content_text = call_nova(LITE_MODEL_ID, messages)\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(\"-\" * 40)\n",
    "print_output(content_text)\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cot = \"\"\"You are a project manager for a small software development team tasked with launching a new app feature.\n",
    "You want to streamline the development process and ensure timely delivery. Draft a project plan\n",
    "\"\"\"\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": no_cot}]}]\n",
    "\n",
    "model_response, content_text = call_nova(LITE_MODEL_ID, messages, max_tokens=1024)\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(\"-\" * 40)\n",
    "print_output(content_text)\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "The above is great but its too long, and you see it keeps going on more. As mentioned this is for Executives, do we want to keep it this long?\n",
    "\n",
    "\n",
    "Lets now give some guiding questions for model to come up with a draft but first do the thinking\n",
    "\n",
    "\n",
    "### With Guided Chain of Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_cot = \"\"\"You are a project manager for a small software development team tasked with launching a new app feature.\n",
    "You want to streamline the development process and ensure timely delivery.\n",
    "Your task is to draft a project plan.\n",
    "\n",
    "But first do some thinking on how you want to structure and go through below questions before starting the draft.\n",
    "Please follow these steps:\n",
    "1. Think about who the audience is (this is for CEOs, CTOs and other executives)\n",
    "2. Think about what to start with\n",
    "3. Think about what Challenges you want to solve with this app\n",
    "4. Think about the Tasks that will be needed to be completed\n",
    "5. Create Milestones\n",
    "6. Monitor Progress and Optimize\n",
    "Explain all your thinking in <thinking></thinking> XML Tags and then write the final copy of project plan for executives in <project_plan></project_plan> XML Tag.\n",
    "\n",
    "Output Schema:\n",
    "<thinking>\n",
    "( thoughts to above questions)\n",
    "</thinking>\n",
    "<project_plan>\n",
    "( project plan)\n",
    "</project_plan>\n",
    "\"\"\"\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": guided_cot}]}]\n",
    "\n",
    "model_response, content_text = call_nova(LITE_MODEL_ID, messages, max_tokens=2048)\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(\"-\" * 40)\n",
    "print_output(content_text)\n",
    "print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
