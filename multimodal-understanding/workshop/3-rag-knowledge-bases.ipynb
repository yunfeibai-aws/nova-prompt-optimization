{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 3 - Creating Multimodal RAG using Amazon Bedrock Knowledge Bases\n",
    "\n",
    "Amazon Bedrock Knowledge Bases leverage Retrieval Augmented Generation (RAG), a technique that harnesses customer data stores to enhance responses generated by foundation models. Knowledge bases allow agents to access existing customer data repositories without extensive administrator overhead. To connect a knowledge base to your data, you specify an S3 bucket as the data source. By employing knowledge bases, applications gain enriched contextual information, streamlining development through a fully-managed RAG solution. This level of abstraction accelerates time-to-market by minimizing the effort of incorporating your data into agent functionality and it optimizes cost by negating the necessity for continuous model retraining to leverage private data.\n",
    "Knowledge Base Preparation\n",
    "\n",
    "## Use Case Description\n",
    "\n",
    "We will imagine that we are a Hotel Booking Assistant who helps user with some relevent information about the hotel such as finding where the Casino is, to what has been the performance of the hotel in certain quarter based on financial reports to basic room and hotel fare info.\n",
    "\n",
    "For this, we would need a RAG System that has these relevent information about a particular hotel. For this exercise, we will assume we are ABC Hotel and we will load some documents which have some information about the hotel. This information will include not just text but also other charts, graph information.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "![./images/RAG-diagram.gif](images/RAG-diagram.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "boto3.setup_default_session(region_name=region_name)\n",
    "\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "bucket_name = f\"mmu-workshop-{account_id}\"\n",
    "tmp_bucket_name = f\"mmu-workshop-tmp-{account_id}\"\n",
    "\n",
    "r = s3_client.list_buckets(Prefix=bucket_name)\n",
    "if r[\"Buckets\"][0][\"Name\"].startswith(bucket_name):\n",
    "    bucket_name = r[\"Buckets\"][0][\"Name\"]\n",
    "    s3_client.put_object(Bucket=bucket_name, Key=\"mm-data/\")\n",
    "    print(f\"Successfully created mm-data/ folder in {bucket_name}\")\n",
    "    print(f\"S3 URI for Data Source: s3://{bucket_name}/mm-data/\")\n",
    "\n",
    "r = s3_client.list_buckets(Prefix=tmp_bucket_name)\n",
    "if r[\"Buckets\"][0][\"Name\"].startswith(tmp_bucket_name):\n",
    "    tmp_bucket_name = r[\"Buckets\"][0][\"Name\"]\n",
    "    print(f\"S3 URI for Multimodal Storage: s3://{tmp_bucket_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### 1. Create the KB\n",
    "\n",
    "**Step 1:** Navigate to the [Amazon Bedrock > Knowledge base > Create knowledge](https://console.aws.amazon.com/bedrock/home#/knowledge-bases/create-knowledge-base) base console as shown:\n",
    "\n",
    "![images/kb/kb_starter.png](images/kb/kb_starter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "**Step 2**: Next, lets select Create Knowledge Base > Knowledge Base with Vector Store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "**Step 3**: Next, lets provide knowledge base details such as KB name, Description, etc.\n",
    "\n",
    "In below illustration, we are giving \n",
    "- **KB Name**: `knowledge-base-hotel-info` and feel free to put some relevent description\n",
    "- **IAM Role**: Let KB create an IAM role with all needed permissions.\n",
    "- **DataSource**: Next we choose S3 as our Data source, where we will add some menus to use as RAG data source later in the notebook.\n",
    "\n",
    "![images/kb/kb_fill_in.png](images/kb/kb_fill_in.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "**Step 4**: **Configure Data Source**\n",
    "\n",
    "Now, lets configure the data source (S3, in this case) for this to happen we need to provide details as described below:\n",
    "\n",
    "- **S3 URI**: The S3 URI where our multimodal files of the dataset are located. Use the `s3://mmu-workshop-*****/mm-data` as bucket and sub-folder path.\n",
    "- For **Parsing Strategy**, select **Foundation models as a parser** and then choose **Claude 3 Haiku v1** as model.\n",
    "\n",
    "This means that Haiku will be used to parse the multimodal content and summarize the images before passing it to generator model, where we will be using Amazon Nova.\n",
    "\n",
    "![images/kb/kb_configure.png](images/kb/kb_configure.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note: S3 URI for Data Source</b>- ⚠️ For S3 location for Data Source choose the bucket where you will store the multimodal PDF files. If you are running this notebook as part of an AWS event using Workshop Studio, you should see a bucket with similar name mmu-workshop-********  and create a partition in there to separate our data such as `mm-data`\n",
    "So the overall S3 URI becomes like:\n",
    "\n",
    "    \n",
    "  `s3://mmu-workshop-********/mm-data`\n",
    "<br>\n",
    "<b>Note: S3 URI for Multimodal Storage</b>- ⚠️ For S3 location for Multimodal Storage(we will show this below) we will create a separate bucket where the parsed images will be stored. If you are using in workshop you should see a bucket with similar name - mmu-workshop-tmp-********\n",
    "So the overall S3 URI becomes like:\n",
    "\n",
    "    \n",
    "  `s3://mmu-workshop-tmp-********`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "**Step 5: Select Embedding Model and Configure Vector Store**\n",
    "\n",
    "- Select the embedding model **Amazon Titan Embedding Model v2**, which will be used to create vector representations of the multimodal input.\n",
    "\n",
    "- For **Vector Database**, keep the default choice of **Open Search Serverless**.\n",
    "\n",
    "- For **Multimodal storage destination**, select the previously discovered S3 bucket for temporary files `mmu-workshop-tmp-******`.\n",
    "\n",
    "![images/kb/kb_embedding.png](images/kb/kb_embedding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "**Step 6: Review and Create the Knowledge Base**\n",
    "\n",
    "Finally, lets review the details enter, and click submit to create a Multimodal Knowledge base\n",
    "\n",
    "Once the Knowledge base is created (this generally takes 4-5 mins) you would see the following message on screen\n",
    "\n",
    "![images/kb/kb_successmsg.png](images/kb/kb_successmsg.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### This overall process takes roughly 10 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Ignore any pip install dependency errors you may witness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade -q -r requirements.txt --ignore-installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import json\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import pprint\n",
    "import random\n",
    "import time\n",
    "from retrying import retry\n",
    "\n",
    "boto3_session = boto3.session.Session()\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### 2. Key Information\n",
    "\n",
    "Once the Knowledge Base is created lets note down the following key information in below cell.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Please make sure to add Knowledge Base Name, Id, Data Source ID and the IAM role name created below\n",
    "</div>\n",
    "\n",
    "![images/kb/kb_id_iam_role.png](images/kb/kb_id_iam_role.png)\n",
    "\n",
    "![images/kb/ds_id.png](images/kb/ds_id.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ⚠️ ⚠️ replace below values with the created Knowledge Base and Data Source\n",
    "kb_name = \"\"\n",
    "kb_id = \"\"\n",
    "ds_id = \"\"\n",
    "kb_iam_role_name = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Verify that all resources are created correctly and ready to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_agent_client = boto3_session.client(\"bedrock-agent\")\n",
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\")\n",
    "\n",
    "get_kb_response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId=kb_id)\n",
    "\n",
    "get_ds_response = bedrock_agent_client.get_data_source(\n",
    "    knowledgeBaseId=kb_id, dataSourceId=ds_id\n",
    ")\n",
    "\n",
    "%store kb_name\n",
    "%store kb_id\n",
    "%store ds_id\n",
    "%store account_id\n",
    "%store bucket_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### 3. Ingest the Hotel Information Files into the S3 location "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Using multiple S3 buckets \n",
    "\n",
    "1. **Data Source Input S3 Bucket**: This s3 bucket will serve as an input for creating our Data Source which will create a Vector Database using OpenSearch Serverless. For this we will use the pre-created bucket of the format\n",
    "`mmu-workshop-<ACCOUNT_ID>-*****`\n",
    "\n",
    "2. **Multimodal Storage Bucket**: This s3 bucket will be used to write and read any extracted images from multimodal documents that needs to be refrenced while answering questions related to images. For this we will use a pre-created bucket of the format\n",
    "`mmu-workshop-tmp-<ACCOUNT_ID>-*****`\n",
    "\n",
    "**Note**: We will be syncing in \"mm-data\" partition, if you are syncing somewhere else please make the appropriate modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data to S3 to the bucket that was configured as a data source to the Knowledge Base\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "\n",
    "def interactive_sleep(seconds: int):\n",
    "    dots = \"\"\n",
    "    for i in range(seconds):\n",
    "        dots += \".\"\n",
    "        print(dots, end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "def uploadDirectory(path, bucket_name, s3_path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            local_file_path = os.path.join(root, file)\n",
    "            s3_key = os.path.join(s3_path, os.path.relpath(local_file_path, path))\n",
    "            # Upload the file with the new S3 key\n",
    "            s3_client.upload_file(local_file_path, bucket_name, s3_key)\n",
    "\n",
    "\n",
    "uploadDirectory(\"kb_data\", bucket_name, \"mm-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### 4. Sync the KB Data Source _[You can also skip this to do from UI by clicking \"Sync Data Source\"]_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "![images/kb/kb_sync.png](images/kb/kb_sync.png)\n",
    "\n",
    "This may take 5-7 mins to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_sleep(30)\n",
    "ingest_jobs = []\n",
    "\n",
    "# Start an ingestion job\n",
    "try:\n",
    "    start_job_response = bedrock_agent_client.start_ingestion_job(\n",
    "        knowledgeBaseId=kb_id, dataSourceId=ds_id\n",
    "    )\n",
    "    job = start_job_response[\"ingestionJob\"]\n",
    "    job_id = job[\"ingestionJobId\"]\n",
    "    print(f\"Ingestion job started successfully. {job_id=}\")\n",
    "\n",
    "    while job[\"status\"] not in [\"COMPLETE\", \"FAILED\", \"STOPPED\"]:\n",
    "        get_job_response = bedrock_agent_client.get_ingestion_job(\n",
    "            knowledgeBaseId=kb_id, dataSourceId=ds_id, ingestionJobId=job_id\n",
    "        )\n",
    "        job = get_job_response[\"ingestionJob\"]\n",
    "    pp.pprint(job)\n",
    "    interactive_sleep(40)\n",
    "    ingest_jobs.append(job)\n",
    "except Exception as e:\n",
    "    print(\"Failed to start ingestion job!\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### 5. Update the IAM Policy to include Amazon Nova as generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_policy_json_to_role(role_name, policy_name, policy_json):\n",
    "    \"\"\"\n",
    "    Attaches a policy JSON directly to an IAM role.\n",
    "\n",
    "    :param role_name: The name of the IAM role\n",
    "    :param policy_name: The name to give the new policy\n",
    "    :param policy_json: The policy document as a JSON string or dictionary\n",
    "    :return: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create an IAM client\n",
    "        iam_client = boto3.client(\"iam\")\n",
    "\n",
    "        # Ensure policy_json is a string\n",
    "        if isinstance(policy_json, dict):\n",
    "            policy_json = json.dumps(policy_json)\n",
    "\n",
    "        # Create the policy\n",
    "        response = iam_client.create_policy(\n",
    "            PolicyName=policy_name, PolicyDocument=policy_json\n",
    "        )\n",
    "\n",
    "        # Get the ARN of the newly created policy\n",
    "        policy_arn = response[\"Policy\"][\"Arn\"]\n",
    "\n",
    "        # Attach the policy to the role\n",
    "        iam_client.attach_role_policy(RoleName=role_name, PolicyArn=policy_arn)\n",
    "\n",
    "        print(\n",
    "            f\"Successfully created policy {policy_name} and attached it to role {role_name}\"\n",
    "        )\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error attaching policy JSON to role: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Example usage\n",
    "policy_name = \"NovaProModelPolicy\"\n",
    "policy_json = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"BedrockInvokeModelStatement\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\"bedrock:*\"],\n",
    "            \"Resource\": [\n",
    "                f\"arn:aws:bedrock:us-east-1::foundation-model/{PRO_MODEL_ID.removeprefix('us.')}\",\n",
    "                f\"arn:aws:bedrock:us-west-2::foundation-model/{PRO_MODEL_ID.removeprefix('us.')}\",\n",
    "                f\"arn:aws:bedrock:us-west-2:{account_id}:inference-profile/{PRO_MODEL_ID}\",\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "attach_policy_json_to_role(kb_iam_role_name, policy_name, policy_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### 6. Test the KB retrieve and retrieve and generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Using RetrieveAndGenerate API\n",
    "Behind the scenes, RetrieveAndGenerate API converts queries into embeddings, searches the knowledge base, and then augments the foundation model prompt with the search results as context information and returns the FM-generated response to the question. For multi-turn conversations, Knowledge Bases manage short-term memory of the conversation to provide more contextual results.\n",
    "\n",
    "The output of the RetrieveAndGenerate API includes the generated response, source attribution as well as the retrieved text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import s3fs\n",
    "import ipywidgets as widgets\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "\n",
    "def ask_bedrock_llm_with_knowledge_base(query: str, model_arn: str, kb_id: str) -> str:\n",
    "    response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "        input={\"text\": query},\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            \"type\": \"KNOWLEDGE_BASE\",\n",
    "            \"knowledgeBaseConfiguration\": {\n",
    "                \"knowledgeBaseId\": kb_id,\n",
    "                \"modelArn\": model_arn,\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "\n",
    "## Function to print retrieved response\n",
    "def print_response(response):\n",
    "    # structure 'retrievalResults': list of contents. Each list has ['ResponseMetadata', 'citations', 'output', 'sessionId']\n",
    "    generated_text = response[\"output\"][\"text\"]\n",
    "    ref_ref_location_lst = []\n",
    "    ref_ref_location_lst.append({\"generated_text\": generated_text})\n",
    "    for num, chunk in enumerate(response[\"citations\"]):\n",
    "        ref_locations = []\n",
    "        for i, ref in enumerate(chunk[\"retrievedReferences\"]):\n",
    "            data_dict = {\n",
    "                \"ref_location\": ref[\"location\"],\n",
    "                \"ref_metadata\": ref[\"metadata\"][\"x-amz-bedrock-kb-source-uri\"],\n",
    "            }\n",
    "            if \"x-amz-bedrock-kb-byte-content-source\" in ref[\"metadata\"].keys():\n",
    "                data_dict[\"ref_image\"] = ref[\"metadata\"][\n",
    "                    \"x-amz-bedrock-kb-byte-content-source\"\n",
    "                ]\n",
    "            ref_locations.append(data_dict)\n",
    "        ref_ref_location_lst.append({\"chunk_details\": ref_locations})\n",
    "    return ref_ref_location_lst\n",
    "\n",
    "\n",
    "def create_tree_widget(data, s3=None):\n",
    "    s3 = s3 or s3fs.S3FileSystem(anon=False)\n",
    "    main_accordion = widgets.Accordion()\n",
    "\n",
    "    for i, item in enumerate(data):\n",
    "        subchildren = []\n",
    "\n",
    "        # Always add Generated Text first\n",
    "        if \"generated_text\" in item:\n",
    "            text_widget = widgets.Textarea(\n",
    "                value=str(item[\"generated_text\"]),\n",
    "                disabled=True,\n",
    "                layout=widgets.Layout(width=\"500px\", height=\"200px\"),\n",
    "            )\n",
    "            subchildren.append(text_widget)\n",
    "\n",
    "        # Then add Chunk Details\n",
    "        if \"chunk_details\" in item:\n",
    "            chunk_accordion = widgets.Accordion()\n",
    "            chunk_children = []\n",
    "\n",
    "            for chunk in item[\"chunk_details\"]:\n",
    "                chunk_subchildren = []\n",
    "\n",
    "                for key, value in chunk.items():\n",
    "                    if (\n",
    "                        key == \"ref_image\"\n",
    "                        and isinstance(value, str)\n",
    "                        and value.startswith(\"s3://\")\n",
    "                    ):\n",
    "                        try:\n",
    "                            with s3.open(value, \"rb\") as f:\n",
    "                                img = PILImage.open(f).resize((400, 400))\n",
    "                                img_byte_arr = io.BytesIO()\n",
    "                                img.save(img_byte_arr, format=\"PNG\")\n",
    "                                img_widget = widgets.Image(\n",
    "                                    value=img_byte_arr.getvalue(),\n",
    "                                    format=\"png\",\n",
    "                                    width=400,\n",
    "                                    height=400,\n",
    "                                )\n",
    "                            chunk_subchildren.append(img_widget)\n",
    "                        except Exception as e:\n",
    "                            chunk_subchildren.append(widgets.Label(f\"Image Error: {e}\"))\n",
    "                    else:\n",
    "                        chunk_subchildren.append(\n",
    "                            widgets.Label(f\"{key}: {json.dumps(value)}\")\n",
    "                        )\n",
    "\n",
    "                chunk_item_accordion = widgets.Accordion(\n",
    "                    children=tuple(chunk_subchildren)\n",
    "                )\n",
    "                for k, child in enumerate(chunk_subchildren):\n",
    "                    chunk_item_accordion.set_title(k, list(chunk.keys())[k])\n",
    "\n",
    "                chunk_children.append(chunk_item_accordion)\n",
    "\n",
    "            chunk_accordion = widgets.Accordion(children=tuple(chunk_children))\n",
    "            for j, child in enumerate(chunk_children):\n",
    "                chunk_accordion.set_title(j, f\"Chunk {j+1}\")\n",
    "\n",
    "            subchildren.append(chunk_accordion)\n",
    "\n",
    "        # Create item accordion with correct titles\n",
    "        item_accordion = widgets.Accordion(children=tuple(subchildren))\n",
    "        item_accordion.set_title(0, \"Generated Text\")\n",
    "        if len(subchildren) > 1:\n",
    "            item_accordion.set_title(1, \"Chunk Details\")\n",
    "\n",
    "        main_accordion.children += (item_accordion,)\n",
    "        main_accordion.set_title(i, f\"Item {i}\")\n",
    "\n",
    "    return main_accordion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Using Textual Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the check-in and check-out time at ABC Grand hotel?\"\n",
    "model_arn = f\"arn:aws:bedrock:{region_name}:{account_id}:inference-profile/{PRO_MODEL_ID}\"\n",
    "\n",
    "response = ask_bedrock_llm_with_knowledge_base(query, model_arn, kb_id)\n",
    "response_data = print_response(response)\n",
    "tree_widget = create_tree_widget(response_data)\n",
    "display(tree_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Using Multimodal search to find information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Check the floor map and see if there is a Fitness Room at ABC Grand? If so, where is it located?\"\n",
    "model_arn = f\"arn:aws:bedrock:{region_name}:{account_id}:inference-profile/{PRO_MODEL_ID}\"\n",
    "\n",
    "response = ask_bedrock_llm_with_knowledge_base(query, model_arn, kb_id)\n",
    "response_data = print_response(response)\n",
    "tree_widget = create_tree_widget(response_data)\n",
    "display(tree_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Chart and Graph Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the revenue in 2022 versus 2023?\"\n",
    "model_arn = f\"arn:aws:bedrock:{region_name}:{account_id}:inference-profile/{PRO_MODEL_ID}\"\n",
    "\n",
    "response = ask_bedrock_llm_with_knowledge_base(query, model_arn, kb_id)\n",
    "response_data = print_response(response)\n",
    "tree_widget = create_tree_widget(response_data)\n",
    "display(tree_widget)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
