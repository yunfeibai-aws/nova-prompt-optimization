{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09c3cdab",
   "metadata": {},
   "source": [
    "# 01_Getting Started with Amazon Nova Models\n",
    "\n",
    "Amazon Nova is a new generation of multimodal understanding and creative content generation models that offer state-of-the-art quality, unparalleled customization, and the best price-performance. Amazon Nova models incorporate the same secure-by-design approach as all AWS services, with built-in controls for the safe and responsible use of AI.\n",
    "\n",
    "Amazon Nova has two categories of models: \n",
    " - **Understanding models** —These models are capable of reasoning over several input modalities, including text, video, and image, and output text. \n",
    "- **Creative Content Generation models** —These models generate images or videos based on a text or image prompt.\n",
    "  \n",
    "### Amazon Nova Models at Glance\n",
    "![media/model_intro.png](media/model_intro.png)\n",
    "\n",
    "**Multimodal Understanding Models**\n",
    "- **Amazon Nova Micro**: Lightening fast, cost-effective text-only model\n",
    "- **Amazon Nova Lite**: Fastest, most affordable multimodal FM in the industry for its intelligence tier\n",
    "- **Amazon Nova Pro**:  The fastest, most cost-effective, state-of-the-art multimodal model in the industry\n",
    "\n",
    "**Creative Content Generation Models**\n",
    "- **Amazon Nova Canvas**:State-of-the-art image generation model\n",
    "- **Amazon Nova Reel**:State-of-the-art video generation model\n",
    "\n",
    "\n",
    "The following notebooks will be focused primarily on Amazon Nova Understanding Models. \n",
    "\n",
    "**Amazon Nova Multimodal understanding** foundation models (FMs) are a family of models that are capable of reasoning over several input modalities, including text, video, documents and/or images, and output text. You can access these models through the Bedrock Converse API and InvokeModel API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aea9407",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Setup\n",
    "\n",
    "**Step 1: Gain Access to the Model**: If you have not yet requested for model access in Bedrock, you do so [request access following these instructions](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access-modify.html).\n",
    "\n",
    "![media/model_access.png](media/model_access.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c633ddd-299c-4fb6-98ba-b01bb8b48f50",
   "metadata": {},
   "source": [
    "## 2 When to Use What?\n",
    "\n",
    "## 2.1 When to Use Amazon Nova Micro Model\n",
    "\n",
    "Amazon Nova Micro (Text Input Only) is the fastest and most affordable option, optimized for large-scale, latency-sensitive deployments like conversational interfaces, chats, and high-volume tasks, such as classification, routing, entity extraction, and document summarization.\n",
    "\n",
    "## 2.2 When to Use Amazon Nova Lite Model\n",
    "\n",
    "Amazon Nova Lite balances intelligence, latency, and cost-effectiveness. It’s optimized for complex scenarios where low latency (minimal delay) is crucial, such as interactive agents that need to orchestrate multiple tool calls simultaneously. Amazon Nova Lite supports image, video, and text inputs and outputs text. \n",
    "\n",
    "## 2.3 When to Use Amazon Nova Pro Model\n",
    "Amazon Nova Pro is designed for highly complex use cases requiring advanced reasoning, creativity, and code generation. Amazon Nova pro supports image, video, and text inputs and outputs text. \n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Run the cells in this section to install the packages needed by the notebooks in this workshop. ⚠️ You will see pip dependency errors, you can safely ignore these errors. ⚠️\n",
    "\n",
    "_IGNORE ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1effee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --no-build-isolation --force-reinstall \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfe86f9-b9d3-4d10-b4c6-d80f38d52b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e9d37a",
   "metadata": {},
   "source": [
    "### 2. Text Understanding [Applicable for Amazon Nova Micro, Amazon Nova Lite, Amazon Nova Pro]\n",
    "\n",
    "The example below demonstrates how to use a text-based prompt, including a simple system prompt and message list.\n",
    "\n",
    "\n",
    "Note: Below examples are using Nova Lite for Illustrative Purposes but you can make use of Micro Models for Text Understanidng Usecases as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dd0c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import base64\n",
    "from datetime import datetime\n",
    "\n",
    "PRO_MODEL_ID = \"us.amazon.nova-pro-v1:0\"\n",
    "LITE_MODEL_ID = \"us.amazon.nova-lite-v1:0\"\n",
    "MICRO_MODEL_ID = \"us.amazon.nova-micro-v1:0\"\n",
    "\n",
    "# Create a Bedrock Runtime client in the AWS Region of your choice.\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e62778-e5f5-4664-a81c-ac27b54d3c2c",
   "metadata": {},
   "source": [
    "### InvokeModel body and output\n",
    "\n",
    "The invoke_model() method of the Amazon Bedrock runtime client (InvokeModel API) will be the primary method we use for most of our Text Generation and Processing tasks\n",
    "\n",
    "Although the method is shared, the format of input and output varies depending on the foundation model used - as described below:\n",
    "\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"system\": [\n",
    "    {\n",
    "      \"text\": string\n",
    "    }\n",
    "  ],\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",# first turn should always be the user turn\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"text\": string\n",
    "        },\n",
    "        {\n",
    "          \"image\": {\n",
    "            \"format\": \"jpeg\"| \"png\" | \"gif\" | \"webp\",\n",
    "            \"source\": {\n",
    "              \"bytes\": \"base64EncodedImageDataHere...\"#  base64-encoded binary\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"video\": {\n",
    "            \"format\": \"mkv\" | \"mov\" | \"mp4\" | \"webm\" | \"three_gp\" | \"flv\" | \"mpeg\" | \"mpg\" | \"wmv\",\n",
    "            \"source\": {\n",
    "            # source can be s3 location of base64 bytes based on size of input file. \n",
    "               \"s3Location\": {\n",
    "                \"uri\": string, #  example: s3://my-bucket/object-key\n",
    "                \"bucketOwner\": string #  (Optional) example: 123456789012)\n",
    "               }\n",
    "              \"bytes\": \"base64EncodedImageDataHere...\" #  base64-encoded binary\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"text\": string # prefilling assistant turn\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    " \"inferenceConfig\":{ # all Optional\n",
    "    \"max_new_tokens\": int, #  greater than 0, equal or less than 5k (default: dynamic*)\n",
    "    \"temperature\": float, # greater then 0 and less than 1.0 (default: 0.7)\n",
    "    \"top_p\": float, #  greater than 0, equal or less than 1.0 (default: 0.9)\n",
    "    \"top_k\": int #  0 or greater (default: 50)\n",
    "    \"stopSequences\": [string]\n",
    "  },\n",
    "  \"toolConfig\": { #  all Optional\n",
    "        \"tools\": [\n",
    "                {\n",
    "                    \"toolSpec\": {\n",
    "                        \"name\": string # menaingful tool name (Max char: 64)\n",
    "                        \"description\": string # meaningful description of the tool\n",
    "                        \"inputSchema\": {\n",
    "                            \"json\": { # The JSON schema for the tool. For more information, see JSON Schema Reference\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    <args>: { # arguments \n",
    "                                        \"type\": string, # argument data type\n",
    "                                        \"description\": string # meaningful description\n",
    "                                    }\n",
    "                                },\n",
    "                                \"required\": [\n",
    "                                    string # args\n",
    "                                ]\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "   \"toolChoice\": \"any\" //Amazon Nova models ONLY support tool choice of \"any\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The following are required parameters.\n",
    "\n",
    "* `system` – (Optional) The system prompt for the request.\n",
    "    A system prompt is a way of providing context and instructions to Amazon Nova, such as specifying a particular goal or role.\n",
    "* `messages` – (Required) The input messages.\n",
    "    * `role` – The role of the conversation turn. Valid values are user and assistant. \n",
    "    * `content` – (required) The content of the conversation turn.\n",
    "        * `type` – (required) The type of the content. Valid values are image, text. , video\n",
    "            * if chosen text (text content)\n",
    "                * `text` - The content of the conversation turn. \n",
    "            * If chosen Image (image content)\n",
    "                * `source` – (required) The base64 encoded image bytes for the image.\n",
    "                * `format` – (required) The type of the image. You can specify the following image formats. \n",
    "                    * `jpeg`\n",
    "                    * `png`\n",
    "                    * `webp`\n",
    "                    * `gif`\n",
    "            * If chosen video: (video content)\n",
    "                * `source` – (required) The base64 encoded image bytes for the video or S3 URI and bucket owner as shown in the above schema\n",
    "                * `format` – (required) The type of the video. You can specify the following video formats. \n",
    "                    * `mkv`\n",
    "                    *  `mov`  \n",
    "                    *  `mp4`\n",
    "                    *  `webm`\n",
    "                    *  `three_gp`\n",
    "                    *  `flv`  \n",
    "                    *  `mpeg`  \n",
    "                    *  `mpg`\n",
    "                    *  `wmv`\n",
    "* `inferenceConfig`: These are inference config values that can be passed in inference.\n",
    "    * `max_new_tokens` – (Optional) The maximum number of tokens to generate before stopping.\n",
    "        Note that Amazon Nova models might stop generating tokens before reaching the value of max_tokens. Maximum New Tokens value allowed is 5K.\n",
    "    * `temperature` – (Optional) The amount of randomness injected into the response.\n",
    "    * `top_p` – (Optional) Use nucleus sampling. Amazon Nova computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.\n",
    "    * `top_k` – (Optional) Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.\n",
    "    * `stopSequences` – (Optional) Array of strings containing step sequences. If the model generates any of those strings, generation will stop and response is returned up until that point. \n",
    "    * `toolConfig` – (Optional) JSON object following ToolConfig schema,  containing the tool specification and tool choice. This schema is the same followed by the Converse API\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a9f067",
   "metadata": {},
   "source": [
    "\n",
    "#### 2.1 Synchronous API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353f4769-4397-4dae-a4de-8a295569f6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your system prompt(s).\n",
    "system_list = [\n",
    "    { \"text\": \"You should respond to all messages in french\" }\n",
    "]\n",
    "\n",
    "# Define one or more messages using the \"user\" and \"assistant\" roles.\n",
    "message_list = [\n",
    "    {\"role\": \"user\", \"content\": [{\"text\": \"tell me a joke\"}]},\n",
    "]\n",
    "\n",
    "# Configure the inference parameters.\n",
    "inf_params = {\"max_new_tokens\": 300, \"top_p\": 0.9, \"top_k\": 20, \"temperature\": 0.7}\n",
    "\n",
    "native_request = {\n",
    "    \"messages\": message_list,\n",
    "    \"system\": system_list,\n",
    "    \"inferenceConfig\": inf_params,\n",
    "}\n",
    "\n",
    "# Invoke the model and extract the response body.\n",
    "response = client.invoke_model(modelId=LITE_MODEL_ID, body=json.dumps(native_request))\n",
    "request_id = response[\"ResponseMetadata\"][\"RequestId\"]\n",
    "print(f\"Request ID: {request_id}\")\n",
    "model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "# Pretty print the response JSON.\n",
    "print(\"\\n[Full Response]\")\n",
    "print(json.dumps(model_response, indent=2))\n",
    "\n",
    "# Print the text content for easy readability.\n",
    "content_text = model_response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(content_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9926bc",
   "metadata": {},
   "source": [
    "#### 2.2 Streaming API Call\n",
    "\n",
    "The example below demonstrates how to use a text-based prompt with the streaming API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4728b5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define your system prompt(s).\n",
    "system_list = [\n",
    "    { \"text\": \"Act as a creative writing assistant. When the user provides you with a topic, write a short story about that topic.\" }\n",
    "]\n",
    "\n",
    "# Define one or more messages using the \"user\" and \"assistant\" roles.\n",
    "message_list = [{\"role\": \"user\", \"content\": [{\"text\": \"A camping trip\"}]}]\n",
    "\n",
    "# Configure the inference parameters.\n",
    "inf_params = {\"max_new_tokens\": 500, \"top_p\": 0.9, \"top_k\": 20, \"temperature\": 0.7}\n",
    "\n",
    "request_body = {\n",
    "    \"messages\": message_list,\n",
    "    \"system\": system_list,\n",
    "    \"inferenceConfig\": inf_params,\n",
    "}\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Invoke the model with the response stream\n",
    "response = client.invoke_model_with_response_stream(\n",
    "    modelId=LITE_MODEL_ID, body=json.dumps(request_body)\n",
    ")\n",
    "\n",
    "request_id = response.get(\"ResponseMetadata\").get(\"RequestId\")\n",
    "print(f\"Request ID: {request_id}\")\n",
    "print(\"Awaiting first token...\")\n",
    "\n",
    "chunk_count = 0\n",
    "time_to_first_token = None\n",
    "\n",
    "# Process the response stream\n",
    "stream = response.get(\"body\")\n",
    "if stream:\n",
    "    for event in stream:\n",
    "        chunk = event.get(\"chunk\")\n",
    "        if chunk:\n",
    "            # Print the response chunk\n",
    "            chunk_json = json.loads(chunk.get(\"bytes\").decode())\n",
    "            # Pretty print JSON\n",
    "            # print(json.dumps(chunk_json, indent=2, ensure_ascii=False))\n",
    "            content_block_delta = chunk_json.get(\"contentBlockDelta\")\n",
    "            if content_block_delta:\n",
    "                if time_to_first_token is None:\n",
    "                    time_to_first_token = datetime.now() - start_time\n",
    "                    print(f\"Time to first token: {time_to_first_token}\")\n",
    "\n",
    "                chunk_count += 1\n",
    "                current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S:%f\")\n",
    "                # print(f\"{current_time} - \", end=\"\")\n",
    "                print(content_block_delta.get(\"delta\").get(\"text\"), end=\"\")\n",
    "    print(f\"Total chunks: {chunk_count}\")\n",
    "else:\n",
    "    print(\"No response stream received.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c25e1a9-912e-4bb5-82ef-9b3f673bd215",
   "metadata": {},
   "source": [
    "### 3. Multimodal Understanding [Applicable only for Amazon Nova lite/Amazon Nova Pro Model]\n",
    "\n",
    "The following examples show how to pass various media types to the model. *(reminder - this is only supported with the Lite model)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0239ff-e022-462f-b44d-b6d840ceade5",
   "metadata": {},
   "source": [
    "#### 3.1 Image Understanding\n",
    "\n",
    "Lets see how Nova model does on Image Understanding Usecase. \n",
    "\n",
    "Here we will pass an Image of a Sunset and ask model to try to create 3 art titles for this image. \n",
    "\n",
    "![A Sunset Image](media/sunset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01e09ea-112d-4cfd-a778-b06f6477be85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Open the image you'd like to use and encode it as a Base64 string.\n",
    "with open(\"media/sunset.png\", \"rb\") as image_file:\n",
    "    binary_data = image_file.read()\n",
    "    base_64_encoded_data = base64.b64encode(binary_data)\n",
    "    base64_string = base_64_encoded_data.decode(\"utf-8\")\n",
    "\n",
    "# Define your system prompt(s).\n",
    "system_list = [\n",
    "    { \"text\": \"You are an expert artist. When the user provides you with an image, provide 3 potential art titles\" }\n",
    "]\n",
    "\n",
    "# Define a \"user\" message including both the image and a text prompt.\n",
    "message_list = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"image\": {\n",
    "                    \"format\": \"png\",\n",
    "                    \"source\": {\"bytes\": base64_string},\n",
    "                }\n",
    "            },\n",
    "            {\"text\": \"Provide art titles for this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Configure the inference parameters.\n",
    "inf_params = {\"max_new_tokens\": 300, \"top_p\": 0.1, \"top_k\": 20, \"temperature\": 0.3}\n",
    "\n",
    "native_request = {\n",
    "    \"messages\": message_list,\n",
    "    \"system\": system_list,\n",
    "    \"inferenceConfig\": inf_params,\n",
    "}\n",
    "\n",
    "# Invoke the model and extract the response body.\n",
    "response = client.invoke_model(modelId=LITE_MODEL_ID, body=json.dumps(native_request))\n",
    "model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "# Pretty print the response JSON.\n",
    "print(\"[Full Response]\")\n",
    "print(json.dumps(model_response, indent=2))\n",
    "\n",
    "# Print the text content for easy readability.\n",
    "content_text = model_response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(content_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12c7267",
   "metadata": {},
   "source": [
    "There can be multiple image contents. In this example we ask the model to find what two images have in common:\n",
    "\n",
    "![](media/cat.jpeg)\n",
    "\n",
    "![](media/dog.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b13815",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Open the image you'd like to use and encode it as a Base64 string.\n",
    "with open(\"media/dog.jpeg\", \"rb\") as image_file:\n",
    "    binary_data = image_file.read()\n",
    "    base_64_encoded_data = base64.b64encode(binary_data)\n",
    "    dog_base64_string = base_64_encoded_data.decode(\"utf-8\")\n",
    "\n",
    "with open(\"media/cat.jpeg\", \"rb\") as image_file:\n",
    "    binary_data = image_file.read()\n",
    "    base_64_encoded_data = base64.b64encode(binary_data)\n",
    "    cat_base64_string = base_64_encoded_data.decode(\"utf-8\")\n",
    "\n",
    "# Define a \"user\" message including both the image and a text prompt.\n",
    "message_list = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"image\": {\n",
    "                    \"format\": \"jpeg\",\n",
    "                    \"source\": {\"bytes\": dog_base64_string},\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"image\": {\n",
    "                    \"format\": \"jpeg\",\n",
    "                    \"source\": {\"bytes\": cat_base64_string},\n",
    "                }\n",
    "            },\n",
    "            {\"text\": \"What do these two images have in common?\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Configure the inference parameters.\n",
    "inf_params = {\"max_new_tokens\": 300, \"top_p\": 0.1, \"top_k\": 20, \"temperature\": 0.3}\n",
    "\n",
    "native_request = {\n",
    "    \"messages\": message_list,\n",
    "    \"inferenceConfig\": inf_params,\n",
    "}\n",
    "\n",
    "# Invoke the model and extract the response body.\n",
    "response = client.invoke_model(modelId=LITE_MODEL_ID, body=json.dumps(native_request))\n",
    "model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "# Pretty print the response JSON.\n",
    "print(\"[Full Response]\")\n",
    "print(json.dumps(model_response, indent=2))\n",
    "\n",
    "# Print the text content for easy readability.\n",
    "content_text = model_response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(content_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2976fe-29dc-477b-b27a-758f8f450a05",
   "metadata": {},
   "source": [
    "#### 3.2 Video Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c0de24-c4cf-497c-b6a7-d38b7c6b963e",
   "metadata": {},
   "source": [
    "Lets now, try to see how Nova does on Video understanding use case\n",
    "\n",
    "Here we are going to pass in a video with Quesdilla making instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f9a327-79b2-472b-911e-735546aa51bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"media/the-sea.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9253bf6e-b2dc-41ef-ae5f-b06e35568eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Open the image you'd like to use and encode it as a Base64 string.\n",
    "with open(\"media/the-sea.mp4\", \"rb\") as video_file:\n",
    "    binary_data = video_file.read()\n",
    "    base_64_encoded_data = base64.b64encode(binary_data)\n",
    "    base64_string = base_64_encoded_data.decode(\"utf-8\")\n",
    "\n",
    "# Define your system prompt(s).\n",
    "system_list = [\n",
    "    { \"text\": \"You are an expert media analyst. When the user provides you with a video, provide 3 potential video titles\" }\n",
    "]\n",
    "\n",
    "# Define a \"user\" message including both the image and a text prompt.\n",
    "message_list = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"video\": {\n",
    "                    \"format\": \"mp4\",\n",
    "                    \"source\": {\"bytes\": base64_string},\n",
    "                }\n",
    "            },\n",
    "            {\"text\": \"Provide video titles for this clip.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Configure the inference parameters.\n",
    "inf_params = {\"max_new_tokens\": 300, \"top_p\": 0.1, \"top_k\": 20, \"temperature\": 0.3}\n",
    "\n",
    "native_request = {\n",
    "    \"messages\": message_list,\n",
    "    \"system\": system_list,\n",
    "    \"inferenceConfig\": inf_params,\n",
    "}\n",
    "\n",
    "# Invoke the model and extract the response body.\n",
    "response = client.invoke_model(modelId=LITE_MODEL_ID, body=json.dumps(native_request))\n",
    "model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "# Pretty print the response JSON.\n",
    "print(\"[Full Response]\")\n",
    "print(json.dumps(model_response, indent=2))\n",
    "\n",
    "# Print the text content for easy readability.\n",
    "content_text = model_response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(content_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a29615-4915-4475-8511-fff08eeb97ad",
   "metadata": {},
   "source": [
    "### Video Understanding using S3 Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e09acca-c6a1-4c76-b9c7-65787e93896a",
   "metadata": {},
   "source": [
    "#### Replace the S3 URI below with the S3 URI where your video is located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b1bc4-3e98-46ed-ab0c-a68e90f7c541",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define your system prompt(s).\n",
    "system_list = [\n",
    "    { \"text\": \"You are an expert media analyst. When the user provides you with a video, provide 3 potential video titles\" }\n",
    "]\n",
    "\n",
    "# Define a \"user\" message including both the image and a text prompt.\n",
    "message_list = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"video\": {\n",
    "                    \"format\": \"mp4\",\n",
    "                    \"source\": {\n",
    "                        \"s3Location\": {\n",
    "                            # Replace the S3 URI\n",
    "                            \"uri\": \"s3://demo-bucket/the-sea.mp4\"\n",
    "                        }\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "            {\"text\": \"Provide video titles for this clip.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Configure the inference parameters.\n",
    "inf_params = {\"max_new_tokens\": 300, \"top_p\": 0.1, \"top_k\": 20, \"temperature\": 0.3}\n",
    "\n",
    "native_request = {\n",
    "    \"schemaVersion\": \"messages-v1\",\n",
    "    \"messages\": message_list,\n",
    "    \"system\": system_list,\n",
    "    \"inferenceConfig\": inf_params,\n",
    "}\n",
    "\n",
    "# Invoke the model and extract the response body.\n",
    "response = client.invoke_model(modelId=LITE_MODEL_ID, body=json.dumps(native_request))\n",
    "model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "# Pretty print the response JSON.\n",
    "print(\"[Full Response]\")\n",
    "print(json.dumps(model_response, indent=2))\n",
    "\n",
    "# Print the text content for easy readability.\n",
    "content_text = model_response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(content_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
